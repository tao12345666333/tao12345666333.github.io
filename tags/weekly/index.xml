<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Weekly on MoeLove</title>
    <link>https://moelove.info/tags/weekly/</link>
    <description>Recent content in Weekly on MoeLove</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh</language>
    <lastBuildDate>Sun, 10 Nov 2019 23:17:45 +0800</lastBuildDate>
    
	<atom:link href="https://moelove.info/tags/weekly/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>K8S 生态周报| Vitess 正式从 CNCF 毕业</title>
      <link>https://moelove.info/2019/11/10/K8S-%E7%94%9F%E6%80%81%E5%91%A8%E6%8A%A5-Vitess-%E6%AD%A3%E5%BC%8F%E4%BB%8E-CNCF-%E6%AF%95%E4%B8%9A/</link>
      <pubDate>Sun, 10 Nov 2019 23:17:45 +0800</pubDate>
      
      <guid>https://moelove.info/2019/11/10/K8S-%E7%94%9F%E6%80%81%E5%91%A8%E6%8A%A5-Vitess-%E6%AD%A3%E5%BC%8F%E4%BB%8E-CNCF-%E6%AF%95%E4%B8%9A/</guid>
      <description>「K8S 生态周报」内容主要包含我所接触到的 K8S 生态相关的每周值得推荐的一些信息。欢迎订阅知乎专栏「k8s生态」。
 Vitess 正式从 CNCF 毕业 CNCF（云原生计算基金会）在美国时间 2019 年 11 月 5 日宣布 Vitess 正式毕业了。
这是 CNCF 中第 8 个正式毕业的项目，最近的几次周报中，基本都会谈到关于 CNCF 项目毕业相关的信息（忙碌的 Q4 啊）
Vitess 最初由 YouTube 在 2010 年创建， 主要是用于 MySQL 横向扩展的数据库系统。据说 Vitess 一直为 YouTube 的所有数据库提供服务，国内貌似是京东使用比较多。
补一张 Vitess 的架构图：
最后，再次 恭喜 Vitess 顺利毕业!
Helm v2.16.0 正式发布 在之前的 K8S 生态周报| Helm v2 最后一个特性版本发布中，我介绍了 Helm 正式发布了 v2.15.0 作为 v2 版本的最后一个特性版本。
而本次发布的 v2.16.0 也确实没有主要的特性更新，都是一些问题修复和安全更新等；（在这次版本中是 13 个独立 committers 之一 :) ）</description>
    </item>
    
    <item>
      <title>K8S 生态周报| Helm v2 爆出全版本漏洞</title>
      <link>https://moelove.info/2019/11/03/K8S-%E7%94%9F%E6%80%81%E5%91%A8%E6%8A%A5-Helm-v2-%E7%88%86%E5%87%BA%E5%85%A8%E7%89%88%E6%9C%AC%E6%BC%8F%E6%B4%9E/</link>
      <pubDate>Sun, 03 Nov 2019 23:17:05 +0800</pubDate>
      
      <guid>https://moelove.info/2019/11/03/K8S-%E7%94%9F%E6%80%81%E5%91%A8%E6%8A%A5-Helm-v2-%E7%88%86%E5%87%BA%E5%85%A8%E7%89%88%E6%9C%AC%E6%BC%8F%E6%B4%9E/</guid>
      <description>「K8S 生态周报」内容主要包含我所接触到的 K8S 生态相关的每周值得推荐的一些信息。欢迎订阅知乎专栏「k8s生态」。
 Jaeger 顺利从 CNCF 毕业 CNCF（云原生计算基金会）在美国时间 2019 年 10 月 31 日宣布 Jaeger 正式毕业了。
这是 CNCF 中第 7 个正式毕业的项目，在上次周报的最后 我才刚提过一些项目提交了毕业申请，也同时以 Jaeger 为时一年的毕业申请申请举了个例子，没想到本周就毕业了。:)
Jaeger 最初是由 Uber 受 Dapper 和 OpenZipkin 启发，开源出来的一套分布式追踪系统。它主要可用于微服务架构下的分布式系统的根因分析，性能/延迟优化，服务依赖等方面。截至目前，它在 GitHub 上有 9.4k 的 star ，986 个 fork 以及 115 位贡献者。
它现在的存储后端主要就支持两种 Cassandra 3.4+ 和 Elasticsearch 5.x/6.x ，在这种系统中，随着服务规模的扩大，对后端存储的要求也会很高。另外，它有一套比较现代化的 UI，是基于 react 开发的。
整体来看的话，在使用上基本技术栈就会成为下面这样：
当然实际的架构也可能会因为基础设施而改变，比如说如果已经使用了 SkyWalking 的话，两者倒是也可以结合，大概就会变成下面这样：
不过，对于技术方案的选择，我个人建议是考虑实际需求，以及对各方案进行理性的权衡，否则的话，说不定什么时候就演变成了下面这样：（这个调用链虽然是可以走通的，但此处我是开个玩笑的， 请勿当真）
最后，再次恭喜 Jaeger 顺利毕业！
Helm 2 爆出全版本受影响的漏洞 本周 Helm 官方披露出来一个全版本 (Helm 在 2.</description>
    </item>
    
    <item>
      <title>K8S 生态周报| Helm v3 最后一个beta版本发布</title>
      <link>https://moelove.info/2019/10/27/K8S-%E7%94%9F%E6%80%81%E5%91%A8%E6%8A%A5-Helm-v3-%E6%9C%80%E5%90%8E%E4%B8%80%E4%B8%AAbeta%E7%89%88%E6%9C%AC%E5%8F%91%E5%B8%83/</link>
      <pubDate>Sun, 27 Oct 2019 23:01:41 +0800</pubDate>
      
      <guid>https://moelove.info/2019/10/27/K8S-%E7%94%9F%E6%80%81%E5%91%A8%E6%8A%A5-Helm-v3-%E6%9C%80%E5%90%8E%E4%B8%80%E4%B8%AAbeta%E7%89%88%E6%9C%AC%E5%8F%91%E5%B8%83/</guid>
      <description>「K8S 生态周报」内容主要包含我所接触到的 K8S 生态相关的每周值得推荐的一些信息。欢迎订阅知乎专栏「k8s生态」。
 Helm v3.0.0-beta.5 发布 这将是最后一个 beta 版本，下一个版本将会是 Helm v3.0.0-rc.1 。现在主要精力都集中在一些 bugfix 上，也会有一些设计方面的事情还需要最终确认。
快速的看一下在此版本中新增的内容：
 为了能提供更多子命令，现在 helm get 和 helm show 分别移动到了 helm get all 和 helm show all， 这是一个破坏性变更; 对 helm get values 增加了一个 --output 的选项，现在支持三种格式 table, json, yaml； helm test 新增了一个 --logs 的参数，这是在 Helm 2 中新增的；  对此版本感兴趣的朋友可以参考 ReleaseNote
Docker Hub 新增双因素认证功能 最近 Docker Hub 上线了 双因素认证 的功能。这个事情主要有两个方面的考虑：
 DockerHub 希望能给用户更多的安全性，以及提供更完善的功能，以此来吸引个人用户/开源项目/组织/企业等使用； 这个事情其实也算是 4 月份被攻击事件的后续，在上个月的周报中，我也介绍了 Docker Hub 上线了 Access Token 的功能。而这次上线 2FA 更是进一步提高了其安全性！  关于如何启用 2FA 可直接参考 Docker Hub 的文档 https://docs.</description>
    </item>
    
    <item>
      <title>K8S 生态周报| Helm v2 最后一个特性版本发布</title>
      <link>https://moelove.info/2019/10/21/K8S-%E7%94%9F%E6%80%81%E5%91%A8%E6%8A%A5-Helm-v2-%E6%9C%80%E5%90%8E%E4%B8%80%E4%B8%AA%E7%89%B9%E6%80%A7%E7%89%88%E6%9C%AC%E5%8F%91%E5%B8%83/</link>
      <pubDate>Mon, 21 Oct 2019 10:01:23 +0800</pubDate>
      
      <guid>https://moelove.info/2019/10/21/K8S-%E7%94%9F%E6%80%81%E5%91%A8%E6%8A%A5-Helm-v2-%E6%9C%80%E5%90%8E%E4%B8%80%E4%B8%AA%E7%89%B9%E6%80%A7%E7%89%88%E6%9C%AC%E5%8F%91%E5%B8%83/</guid>
      <description>「K8S 生态周报」内容主要包含我所接触到的 K8S 生态相关的每周值得推荐的一些信息。欢迎订阅知乎专栏「k8s生态」。 文末有活动，欢迎参与。
 Docker 19.03.4 正式发布 在本周 Docker 发布了 19.03.4 版本，这个版本主要是为了修复上周周报中介绍的 DOCKER-USER iptables 链丢失的问题。
如果要升级 Docker 版本的话，可选择升级到此版本。
Kubernetes 修复全版本影响漏洞 上周周报中的 上游进展 部分，介绍了对 CVE-2019-11253 的修复，限制 YAML/JSON 的解码大小为 3M 。本周相继发布了以下版本，包含了对此漏洞的修复。
 v1.13.12 v1.14.8 v1.15.5 v1.16.2  实际受此漏洞影响的版本是：
 Kubernetes v1.0.0-1.12.x Kubernetes v1.13.0-1.13.11 (修复于 v1.13.12) Kubernetes v1.14.0-1.14.7 (修复于 v1.14.8) Kubernetes v1.15.0-1.15.4 (修复于 v1.15.5) Kubernetes v1.16.0-1.16.1 (修复于 v1.16.2)  建议对集群进行升级。 但是升级前，请务必先阅读完 https://github.com/kubernetes/kubernetes/issues/83253 的内容 在清楚了解不同版本的行为后，再做升级。
对此漏洞感兴趣的朋友，也可参阅社区公告
Prometheus Pushgateway v1.0 正式发布 Prometheus Pushgateway v1.</description>
    </item>
    
    <item>
      <title>K8S 生态周报| Docker 19.03.3 DNS 不再区分大小写</title>
      <link>https://moelove.info/2019/10/13/K8S-%E7%94%9F%E6%80%81%E5%91%A8%E6%8A%A5-Docker-19.03.3-DNS-%E4%B8%8D%E5%86%8D%E5%8C%BA%E5%88%86%E5%A4%A7%E5%B0%8F%E5%86%99/</link>
      <pubDate>Sun, 13 Oct 2019 22:42:59 +0800</pubDate>
      
      <guid>https://moelove.info/2019/10/13/K8S-%E7%94%9F%E6%80%81%E5%91%A8%E6%8A%A5-Docker-19.03.3-DNS-%E4%B8%8D%E5%86%8D%E5%8C%BA%E5%88%86%E5%A4%A7%E5%B0%8F%E5%86%99/</guid>
      <description>「K8S 生态周报」内容主要包含我所接触到的 K8S 生态相关的每周值得推荐的一些信息。欢迎订阅知乎专栏「k8s生态」。 文末有活动，欢迎参与。
 Docker 19.03.3 正式发布 在本周 Docker 发布了 19.03.3 版本，这个版本的变更内容 很重要，我会将主要内容都列出来。(上周周报介绍了 19.03.3-rc1 的一些情况)
已知问题 DOCKER-USER iptables 链丢失；如果你并不需要在 DOCKER-USER 链上定义规则的话，那你也并不会受此问题的影响。
临时解决办法：手动添加丢失的链，操作如下：
iptables -N DOCKER-USER iptables -I FORWARD -j DOCKER-USER iptables -A DOCKER-USER -j RETURN  这个问题会在 19.03.4 中进行修复, 很快会进行发布； 实际会把 libnetwork 中有问题的那段代码先去掉。 如果已经升级了此版本的用户，受到此问题影响的话，可以使用上述方式进行临时解决。
安全问题  将 runc 更新到了 v1.0.0-rc8-92-g84373aaa 这其中包含了 runc 中对 CVE-2017-18367 的修复，该漏洞的根本原因在于 libseccomp-golang 中一个错误的逻辑运算 ，有兴趣的朋友可以点开链接看看实际的修复代码，并且也可以发现该代码其实在 2017 年 4 月就已经合并进 libseccomp-golang 的主干中了，但实际上在今年 6 月在 runc 中才真正修复。  这个问题其实反映出来的是当我们在维护项目时，对自己所用的各种依赖需要有所了解和把握，整体来讲，尽可能避免依赖项过旧是个好事儿；并且安全问题非常值得关注。</description>
    </item>
    
    <item>
      <title>K8S 生态周报| runc v1.0.0-rc9 发布</title>
      <link>https://moelove.info/2019/10/06/K8S-%E7%94%9F%E6%80%81%E5%91%A8%E6%8A%A5-runc-v1.0.0-rc9-%E5%8F%91%E5%B8%83/</link>
      <pubDate>Sun, 06 Oct 2019 23:07:49 +0800</pubDate>
      
      <guid>https://moelove.info/2019/10/06/K8S-%E7%94%9F%E6%80%81%E5%91%A8%E6%8A%A5-runc-v1.0.0-rc9-%E5%8F%91%E5%B8%83/</guid>
      <description>「K8S 生态周报」内容主要包含我所接触到的 K8S 生态相关的每周值得推荐的一些信息。欢迎订阅知乎专栏「k8s生态」。
 runc v1.0.0-rc9 发布 不知不觉，runc v1.0.0-rc9 于近日发布了。早先关注过我文章的朋友们应该看到过我从去年开始每次在 runc 新版本发布时都有专门写一篇文章进行介绍。这次版本的定位主要是修复 CVE-2019-16884 所以我也就不再单独写文章介绍了（另一个原因是现在在假期，还是多抽空陪陪家人）
先对 CVE-2019-16884 做个简单的介绍。这是一个中等级别的漏洞，其主要影响是 runc 源码中的 libcontainer/rootfs_linux.go 在文件挂载至 /proc 时，少做了一些检查，可绕过 AppArmor 的限制，所以可能导致被一些恶意镜像所利用。
主要的修复方式是将原先的 checkMountDestination 函数改写为 checkProcMount，并在其中添加了对源文件类型的判断，只允许 procfs 类型的文件挂载至 /proc 。
此漏洞影响到的范围是 runc 以及一些使用 runc 作为基础组件的容器管理软件。请尽快进行升级。
此版本的地址是：https://github.com/opencontainers/runc/releases/tag/v1.0.0-rc9
Docker v19.03.3-rc1 发布 自从 Docker 修改维护周期后，Docker 对软件的质量要求有了显著提高，每次版本发布前会经历多阶段的测试和回归，确保软件没有什么问题后才会发布正式版。
按现在的进度来看 v19.03.3 应该会在一两周内放出。新版本会将 containerd 升级至最新的 1.2.10 ，并修复了一个在 5.2 版本内核上 overlay2 文件系统挂载时的错误。
关于此版本感兴趣的朋友可以参考 ReleaseNote
上游进展 Kubernetes v1.17 已经进入发布周期，这个版本的发布周期会比较短，现在已经发布了 v1.17.0-alpha 版本，计划是在 12 月 9 日可以最终发布。(想想看，距现在也就两个月的时间，你的集群现在是哪个版本呢？)</description>
    </item>
    
    <item>
      <title>K8S 生态周报| containerd v1.3 正式发布</title>
      <link>https://moelove.info/2019/09/29/K8S-%E7%94%9F%E6%80%81%E5%91%A8%E6%8A%A5-containerd-v1.3-%E6%AD%A3%E5%BC%8F%E5%8F%91%E5%B8%83/</link>
      <pubDate>Sun, 29 Sep 2019 21:44:09 +0800</pubDate>
      
      <guid>https://moelove.info/2019/09/29/K8S-%E7%94%9F%E6%80%81%E5%91%A8%E6%8A%A5-containerd-v1.3-%E6%AD%A3%E5%BC%8F%E5%8F%91%E5%B8%83/</guid>
      <description>「K8S 生态周报」内容主要包含我所接触到的 K8S 生态相关的每周值得推荐的一些信息。欢迎订阅知乎专栏「k8s生态」。
 containerd v1.3.0 正式发布 在上个主版本（v1.2.0）后经过 11 个月，我们终于迎来了 containerd v1.3.0 的发布，当然这个版本也是 containerd 自 CNCF 毕业后的首个主版本。（关于 containerd 毕业的信息可参考我之前的文章）
在三周前 containerd v1.3.0-rc.0 发布时，我也提到过一些，现在我们来整体看一下。
 增加了 Windows v2 的 runtime API， 同时移除了 Windows v1 API； 为修复 CVE-2019-16884 更新了 runc 依赖；
 可配置的插件目录；
 允许插件注册为一个 TCP Server ，通过这种机制其实可以做到很多的事情了；
 增加了流式处理的插件，允许在解包的时候处理自定义资源类型（设计上考虑可使用此功能进行加解密相关的逻辑）；
 支持跨仓库推送镜像；
 新增了 devicemapper 的快照支持，然而需要注意的是 Docker 已经将 devicemapper 的存储驱动标记为了过期，推荐大家使用 Overlay2 的存储驱动；
 在 CRI 方面 io.containerd.runtime.v1.linux 仍然是默认的运行时，可选配置为新的 io.containerd.runc.v2，新的版本</description>
    </item>
    
    <item>
      <title>K8S 生态周报| Kubernetes v1.16 正式发布</title>
      <link>https://moelove.info/2019/09/22/K8S-%E7%94%9F%E6%80%81%E5%91%A8%E6%8A%A5-Kubernetes-v1.16-%E6%AD%A3%E5%BC%8F%E5%8F%91%E5%B8%83/</link>
      <pubDate>Sun, 22 Sep 2019 23:45:37 +0800</pubDate>
      
      <guid>https://moelove.info/2019/09/22/K8S-%E7%94%9F%E6%80%81%E5%91%A8%E6%8A%A5-Kubernetes-v1.16-%E6%AD%A3%E5%BC%8F%E5%8F%91%E5%B8%83/</guid>
      <description>「K8S 生态周报」内容主要包含我所接触到的 K8S 生态相关的每周值得推荐的一些信息。欢迎订阅知乎专栏「k8s生态」。
 Kubernetes v1.16 正式发布 正如我在上次周报中所说，本周 Kubernetes v1.16 正式发布了。此版本中共包含 31 项功能增强：其中 8 项处于 stable 阶段，8 项处于 beta 阶段，剩下的 15 项处于 alpha 阶段。
如果一直关注本周报系列文章的话，这个版本中比较重要的功能我基本都已经提到过了（这里划个重点）。
 CRD 达到 GA ，这是当前社区最为推崇的一种扩展 Kubernetes 的方式，并且自从 1.7 加入后，也被越来越广泛的使用了； 准入控制 webhooks 达到 GA ，准入控制在 Kubernetes 中太过于重要了，自 1.9 该功能加入以来，被广泛用于扩展 Kubernetes 相关功能； 现在 CSI 规范中支持调整卷大小，当前正在迁移至 Beta 阶段； IPv4/IPv6 双栈支持； 为了更好的控制 kube-apiserver 的网络流量，正在尝试给它增加一个代理，详情可点击链接查看；  其余还有一些比较重要的内容：
 现在 kubeadm 在 TLS bootstrap 之后，将会删除 bootstrap-kubelet.conf，如果有依赖此文件的小伙伴，请尽快迁移使用 kubelet.conf ，此外也建议先看看 RBAC 相关的内容，了解下切换的意义； beta.</description>
    </item>
    
    <item>
      <title>K8S 生态周报| Istio 1.3 正式发布</title>
      <link>https://moelove.info/2019/09/15/K8S-%E7%94%9F%E6%80%81%E5%91%A8%E6%8A%A5-Istio-1.3-%E6%AD%A3%E5%BC%8F%E5%8F%91%E5%B8%83/</link>
      <pubDate>Sun, 15 Sep 2019 23:45:23 +0800</pubDate>
      
      <guid>https://moelove.info/2019/09/15/K8S-%E7%94%9F%E6%80%81%E5%91%A8%E6%8A%A5-Istio-1.3-%E6%AD%A3%E5%BC%8F%E5%8F%91%E5%B8%83/</guid>
      <description>「K8S 生态周报」内容主要包含我所接触到的 K8S 生态相关的每周值得推荐的一些信息。欢迎订阅知乎专栏「k8s生态」。
 Istio 1.3.0 正式发布 上周 k8s 生态周报中，我推送了关于 Istio 1.3.0-rc2 发布的消息后，有小伙伴专门私聊我，说想问问 Istio 1.3 到底有什么新特性；以及为何上次没有对 Istio 1.3 的新特性进行介绍。
这里我来做下说明，首先关于为何上次没有对 Istio 1.3 新特性进行介绍。有两个主要原因：1. 上周时，正式版尚未发布；2. 对 1.3 这个版本而言没有太多新特性，此版本主要在于改善用户体验。
对 Istio 而言，今年是个很重要的节点，而且自从 3 月份发布 1.1 版本以来， Istio 的更新频率基本稳定在了 3 个月发布一个版本。1.1 版本专注于企业就绪，在此版本中一方面是提升系统的稳定性，另一方面则是解决企业落地时，可能遇到的一些问题，所以 1.1 中有大量的新特性。而 1.2 版本其实也类似，虽然花费了很多精力在保证质量上，但其中也有不少功能从 Beta 到了 Stable 阶段。
其次是关于 1.3 版本到底有哪些新特性：
 出站流量自动确定协议：之前版本中，Istio 要求 Service 需要按照指定的规则进行命名才可以自动确认其协议，而在此版本中则可以自动确认其是 HTTP 或 HTTP/2 流量，如果无法自动确认，则认为其是纯 TCP 流量，如果是通过 Helm 安装的话，可以使用 --set pilot.enableProtocolSniffing=false 关闭此功能； Pod spec 中不再需要定义 containerPort，默认情况下会捕获所有端口，当然你也可以通过 traffic.</description>
    </item>
    
    <item>
      <title>K8S 生态周报| Harbor v1.9 带来众多新特性</title>
      <link>https://moelove.info/2019/09/08/K8S-%E7%94%9F%E6%80%81%E5%91%A8%E6%8A%A5-Harbor-v1.9-%E5%B8%A6%E6%9D%A5%E4%BC%97%E5%A4%9A%E6%96%B0%E7%89%B9%E6%80%A7/</link>
      <pubDate>Sun, 08 Sep 2019 22:43:44 +0800</pubDate>
      
      <guid>https://moelove.info/2019/09/08/K8S-%E7%94%9F%E6%80%81%E5%91%A8%E6%8A%A5-Harbor-v1.9-%E5%B8%A6%E6%9D%A5%E4%BC%97%E5%A4%9A%E6%96%B0%E7%89%B9%E6%80%A7/</guid>
      <description>「K8S 生态周报」内容主要包含我所接触到的 K8S 生态相关的每周值得推荐的一些信息。欢迎订阅知乎专栏「k8s生态」。
 Docker CE 19.03.2 发布 Docker CE 19.03.2 已于近日发布，事实上此版本内更新的内容本该在 19.03.1 中发布的，不过 19.03.1 主要是为了修正 CVE-2019-14271 如果你正在使用 19.03.0 那我建议你进行升级。
为了保证尽可能快的修复问题，所以专程发布了 19.03.1 版本，而把原先预期的功能转移至 19.03.2 中进行发布。这其实也是 Docker 做的比较好的一个事情，Docker 的版本发布很有原则。
我们来看看 19.03.2 中带来了哪些变化：
 修正了Docker CLI 对 HTTP Proxy 环境变量的支持； 修正了一个为容器使用 XFS 磁盘配额可能产生的 panic；  其他变化，感兴趣的朋友可以参看其 ReleaseNote
containerd 1.3.0-rc.0 发布 1.3 将会是 containerd 的下一个主版本；而自 containerd 1.2 发布以来已经过去了近 9 个月。我们来大致看看 1.3 中有哪些值得期待的功能。
 增加了 Windows v2 的 runtime API， 同时移除了 Windows v1 API； 新增了 devicemapper 的快照支持，然而需要注意的是 Docker 已经将 devicemapper 的存储驱动标记为了过期，推荐大家使用 Overlay2 的存储驱动； 允许插件注册为一个 TCP Server ，通过这种机制其实可以做到很多的事情了； 可配置的插件目录； 在 CRI 方面 io.</description>
    </item>
    
    <item>
      <title>K8S 生态周报| etcd v3.4.0 带来众多更新</title>
      <link>https://moelove.info/2019/09/01/K8S-%E7%94%9F%E6%80%81%E5%91%A8%E6%8A%A5-etcd-v3.4.0-%E5%B8%A6%E6%9D%A5%E4%BC%97%E5%A4%9A%E6%9B%B4%E6%96%B0/</link>
      <pubDate>Sun, 01 Sep 2019 23:56:38 +0800</pubDate>
      
      <guid>https://moelove.info/2019/09/01/K8S-%E7%94%9F%E6%80%81%E5%91%A8%E6%8A%A5-etcd-v3.4.0-%E5%B8%A6%E6%9D%A5%E4%BC%97%E5%A4%9A%E6%9B%B4%E6%96%B0/</guid>
      <description>「K8S 生态周报」内容主要包含我所接触到的 K8S 生态相关的每周值得推荐的一些信息。欢迎订阅知乎专栏「k8s生态」。
 Helm 3 beta2 发布 Helm 3 从 Alpha 之后，就一直进入了持续改进阶段。终于现在 beta2 发布了，按现在社区的开发进度来看，今年发布正式版的希望还是很大的。
感兴趣还是建议可以先尝试下，以免之后升级时带来不适。
CoreDNS v1.6.3 发布  federation 将在 1.7.0 中被完全废弃； 新增两个插件 clouddns 和 sign，其中 clouddns 顾名思义是为云环境设计的，现在它支持 GCP （Google Cloud Platform）Cloud DNS 提供的 zone 数据，实际上它是通过 Google Cloud 的 API 来获取这些信息的，如果你没有在使用 GCP Cloud DNS 的话，目前这个插件应该是用不到的；sign 插件则是根据 RFC 6781 对 Zone 使用 NSEC 签名，但需要注意的是签名是有时效的，如果到了过期时间，则 Zone 信息会变成 Bad 状态（RFC 4035），所以如果你想要使用这个插件，请明确知道自己需要做什么以及为何使用它； file 插件修复了一些内存泄漏的问题；  除了上述提到的内容外，想稍微再提一下在 v1.6.2 中新增的 azure 插件，它其实和 clouddns 做的事情类似，只不过是从 Azure 获取记录罢了。另外从 v1.</description>
    </item>
    
    <item>
      <title>K8S 生态周报| cilium 1.6 发布 100% kube-proxy 的替代品</title>
      <link>https://moelove.info/2019/08/25/K8S-%E7%94%9F%E6%80%81%E5%91%A8%E6%8A%A5-cilium-1.6-%E5%8F%91%E5%B8%83-100-kube-proxy-%E7%9A%84%E6%9B%BF%E4%BB%A3%E5%93%81/</link>
      <pubDate>Sun, 25 Aug 2019 21:19:10 +0800</pubDate>
      
      <guid>https://moelove.info/2019/08/25/K8S-%E7%94%9F%E6%80%81%E5%91%A8%E6%8A%A5-cilium-1.6-%E5%8F%91%E5%B8%83-100-kube-proxy-%E7%9A%84%E6%9B%BF%E4%BB%A3%E5%93%81/</guid>
      <description>「K8S 生态周报」内容主要包含我所接触到的 K8S 生态相关的每周值得推荐的一些信息。欢迎订阅知乎专栏「k8s生态」。
 Kind(Kubernetes In Docker) v0.5.1 正式发布 Kind(Kubernetes In Docker) 已经广泛的应用于 Kubernetes 上游及相关项目的 CI 环境中，作为个人本地的测试环境也很方便，推荐大家尝试。
本次发布，将默认的 Kubernetes 版本更新为 v1.15.3 ；支持了 UDP 和 SCTP 协议的端口转发；对构建 Node 镜像进行了优化，使它更快；同时也对 arm32 增加了有限的支持。
对 kind load-image 进行了改进，从原先的只是判断镜像名称和 tag 到现在增加了对哈希值的校验；修正了在使用 Proxy 时，部分服务可能受代理影响导致的问题（对国内用户友好）。
更多关于此版本的内容，请参考 ReleaseNote，欢迎使用和反馈。
Kubernetes 受 Go 的 net/http 安全漏洞影响 Kubernetes 近期紧急发布了 v1.15.3, v1.14.6, v1.13.10 版本，距离上个集体更新发布仅过了两周而已，上次的说明请参考两周前的 k8s 生态周报，不过本次的漏洞的根本原因不在 Kubernetes 的功能逻辑上，还是在于其使用的 Go 语言的 net/http 库的安全漏洞 CVE-2019-9512 和 CVE-2019-9514 。
关于此次漏洞的信息，可参考 golang/go#33606，另外 Go 最近陆续发布了几个版本，建议大家也最好升到 v1.</description>
    </item>
    
    <item>
      <title>K8S 生态周报| 2019-08-12~2019-08-18</title>
      <link>https://moelove.info/2019/08/19/K8S-%E7%94%9F%E6%80%81%E5%91%A8%E6%8A%A5-2019-08-12~2019-08-18/</link>
      <pubDate>Mon, 19 Aug 2019 01:08:00 +0800</pubDate>
      
      <guid>https://moelove.info/2019/08/19/K8S-%E7%94%9F%E6%80%81%E5%91%A8%E6%8A%A5-2019-08-12~2019-08-18/</guid>
      <description>「K8S 生态周报」内容主要包含我所接触到的 K8S 生态相关的每周值得推荐的一些信息。欢迎订阅知乎专栏「k8s生态」。
 rkt 项目正式被 CNCF 归档 8 月 16 日，CNCF 宣布技术委员会已经投票通过将 rkt 项目归档。
这个事情，我在几周前的周报大概介绍过，既然现在已经尘埃落定，不如一起来看看 rkt 的前世今生，毕竟它在容器技术的发展中也曾做出了很多贡献。
rkt 最早是由 CoreOS 公司创建的，而 CoreOS 最早应该也算是 Docker 的用户之一。但是随着 Docker 发展的日趋壮大，CoreOS 就想要脱离 Docker，成立自己的标准。
之后 CoreOS 发布了 AppC 规范（这个规范我也曾仔细研究过），而 CoreOS 主打的旗号是开放，毕竟当时 Docker 一枝独秀，所以也就吸引了不少的伙伴参与。当然 rkt 也就借着这股风，得到了不少人的青睐。
这里且不说 rkt 功能或是规范如何，我们单独来看看那场容器市场份额的争夺战是如何打的。
在那时候，同时进行的另外一场战役是容器编排系统的战役。所以 rkt 也算是做了努力，它选择了与 Kubernetes 的合作（大概算是共同阵营的合作吧），所以在 2016 年 Kubernetes 1.3 版本中，宣布了支持 rkt 作为容器运行时的一个可选想。
但 Docker 的发展（指 Docker 项目），却几乎没有受到影响。为什么呢？ Docker 早已凭借自身的稳定性和易用性占领了大批的用户，即使有用户选择 rkt 大多也只是用于尝试（心疼选择 rkt 放入生产环境中的那批）。而且不得不说，Docker 在用户心中几乎是容器的代名词，是一个默认选项。</description>
    </item>
    
    <item>
      <title>K8S 生态周报| 2019-08-05~2019-08-11</title>
      <link>https://moelove.info/2019/08/12/K8S-%E7%94%9F%E6%80%81%E5%91%A8%E6%8A%A5-2019-08-05~2019-08-11/</link>
      <pubDate>Mon, 12 Aug 2019 07:14:40 +0800</pubDate>
      
      <guid>https://moelove.info/2019/08/12/K8S-%E7%94%9F%E6%80%81%E5%91%A8%E6%8A%A5-2019-08-05~2019-08-11/</guid>
      <description>「K8S 生态周报」内容主要包含我所接触到的 K8S 生态相关的每周值得推荐的一些信息。欢迎订阅知乎专栏「k8s生态」。
 Kubernetes 两个重要漏洞修复 最近 Kubernetes 发布了 1.13.9, 1.14.5, 和 1.15.2 版本，旨在修复两个重要漏洞对 Kubernetes 带来的影响， 强烈建议将集群及 kubectl 进行升级
CVE-2019-11247 简单来说其影响就是可以访问单个命名空间中的自定义资源的用户可以访问具有集群范围的自定义资源。当然，这里的修正主要是 针对 CRD 。核心的修正代码如下：
var possiblyAcrossAllNamespacesVerbs = sets.NewString(&amp;quot;list&amp;quot;, &amp;quot;watch&amp;quot;) namespacedCRD, namespacedReq := crd.Spec.Scope == apiextensions.NamespaceScoped, len(requestInfo.Namespace) &amp;gt; 0 if !namespacedCRD &amp;amp;&amp;amp; namespacedReq { r.delegate.ServeHTTP(w, req) return } if namespacedCRD &amp;amp;&amp;amp; !namespacedReq &amp;amp;&amp;amp; !possiblyAcrossAllNamespacesVerbs.Has(requestInfo.Verb) { r.delegate.ServeHTTP(w, req) return }  当未通过检查时，delegate 将会触发一个 404 。对此问题感兴趣的朋友可以查看 #80983 。
CVE-2019-11249 则是对于之前暴出来的使用 kubectl cp 可进行恶意目录浏览的漏洞 CVE-2019-1002101 和 CVE-2019-11246 的不完整修复。有兴趣可以参考 #80436 。</description>
    </item>
    
    <item>
      <title>K8S 生态周报| 2019-07-29~2019-08-04</title>
      <link>https://moelove.info/2019/08/05/K8S-%E7%94%9F%E6%80%81%E5%91%A8%E6%8A%A5-2019-07-29~2019-08-04/</link>
      <pubDate>Mon, 05 Aug 2019 00:47:54 +0800</pubDate>
      
      <guid>https://moelove.info/2019/08/05/K8S-%E7%94%9F%E6%80%81%E5%91%A8%E6%8A%A5-2019-07-29~2019-08-04/</guid>
      <description>「K8S 生态周报」内容主要包含我所接触到的 K8S 生态相关的每周值得推荐的一些信息。欢迎订阅知乎专栏「k8s生态」。
 containerd 1.3.0-beta.0 发布 containerd 2014 年从 Docker 孵化出来，最初是作为 Docker 引擎的底层管理器；在 2017 年 3 月被 CNCF 接受后，containerd 几乎成为了行业容器运行引擎的标准，它专注于简单，健壮和可移植性，任何人都可以使用它来构建自己的容器引擎/平台。它是从 CNCF 毕业的第 5 个项目，目前发展势头良好。
本次发布的 1.3.0-beta.0 版本是 containerd 的第 4 个主要版本，主要是为了提升项目的稳定性，以及为了保持项目的活力而持续加入了很多新的特性。
这次的发布和之前版本类似，保持着 containerd 的一贯作风，API 变化很小；并且也保持向后兼容。插件生态和用户的发展也促使了 containerd 变得更易用，可配置和更灵活。
在 Windows 上，此次版本带来了一个新运行时（使用 hcsshim）; 对于客户端而言，本次也带来了很多特性和升级。
这里我只说两点，其余的等正式版出来看情况再进行介绍。
 增加了 devicemapper 的快照插件。这个功能本身是个好事儿，如果用过旧版本 Docker 或者系统内核较低的朋友们，应该对 Docker 的 devicemapper 存储驱动不会太陌生的（虽然现在 Docker 的新版本中已经将 devicemapper 的存储驱动废弃掉了）；至于 containerd 中增加的 devicemapper 快照插件，我还没有来得及具体测试，所以这里不说太多了。 客户端支持了跨 repository push 镜像，对此功能感兴趣的朋友可以参考 #2697 的讨论。  更多关于此版本的信息请参考 ReleaseNote</description>
    </item>
    
    <item>
      <title>K8S 生态周报| 2019-07-21~2019-07-28</title>
      <link>https://moelove.info/2019/07/28/K8S-%E7%94%9F%E6%80%81%E5%91%A8%E6%8A%A5-2019-07-21~2019-07-28/</link>
      <pubDate>Sun, 28 Jul 2019 22:14:44 +0800</pubDate>
      
      <guid>https://moelove.info/2019/07/28/K8S-%E7%94%9F%E6%80%81%E5%91%A8%E6%8A%A5-2019-07-21~2019-07-28/</guid>
      <description>「K8S 生态周报」内容主要包含我所接触到的 K8S 生态相关的每周值得推荐的一些信息。欢迎订阅知乎专栏「k8s生态」。
 Docker CE v19.03 正式发布 7 月 22 日，正式发布了 Docker CE v19.03 版本，按照 Docker CE 版本的生命周期，此次的 v19.03 可以说是千呼万唤始出来，原本按语义应该是在 3 月，不过之前的发布计划中开始是设定在了 5 月份，而一转眼现在已经到 7 月底了。 先跳过这次发布时间延期的问题，我们来看看此版本中最值得注意的一些变化。
首先来看看被废弃的部分：
 废弃 aufs 存储驱动，在此版本之前 devicemapper 和 overlay 也都已经被废弃; 当 docker daemon 运行时，如果没有指定存储驱动，则会自动选择一个，v19.03 中增加了自动选择时跳过已被废弃的存储驱动的逻辑； 废弃 image manifest v2 schema1 以支持 v2 schema2 ，这里的废弃涉及到的内容很多，尤其是涉及到了 image registry 的部分, 所以后续还有很长的路要走。还记得之前推送过 Docker Hub 今年 6 月份停止 v1 API 进行 Pull 操作的事情吗？早年 2015 年 11 月的时候，它就已经禁止了 v1 API 的 Push 操作。从这点上也能看到 Docker 在功能弃用上其实为用户考虑了很多，并且也给了足够长的时间来让用户进行迁移。  其次，我们看看功能增强的部分：</description>
    </item>
    
    <item>
      <title>K8S 生态周报| 2019-07-15~2019-07-21</title>
      <link>https://moelove.info/2019/07/22/K8S-%E7%94%9F%E6%80%81%E5%91%A8%E6%8A%A5-2019-07-15~2019-07-21/</link>
      <pubDate>Mon, 22 Jul 2019 00:07:54 +0800</pubDate>
      
      <guid>https://moelove.info/2019/07/22/K8S-%E7%94%9F%E6%80%81%E5%91%A8%E6%8A%A5-2019-07-15~2019-07-21/</guid>
      <description>「K8S 生态周报」内容主要包含我所接触到的 K8S 生态相关的每周值得推荐的一些信息。欢迎订阅知乎专栏「k8s生态」。
 Kubernetes v1.16.0-alpha.1 发布 Kubernetes 于近日发布了 v1.16.0-alpha.1 版本，变化是比较大，但这里暂时先不细说了，等到 9 月份正式版本发布前后再慢慢说。当然也稍微聊一些 :) 比如：
官方 etcd 镜像中不再提供 etcd 2 和 3 的兼容工具了，对 etcd 2 的兼容代码也都全部删掉了（对 etcd 2 的支持其实从 1.13 就已经停止了）#80037 ；
1.16 中对以下四种类型资源的 API 有所调整：
 NetworkPolicy PodSecurityPolicy DaemonSet, Deployment, StatefulSet 和 ReplicaSet Ingress  具体调整细节如下：
 NetworkPolicy 将使用从 v1.8 版本开始提供的 networking.k8s.io/v1 API; PodSecurityPolicy 将使用从 v1.10 开始提供的 policy/v1beta1 API; DaemonSet, Deployment, StatefulSet 和 ReplicaSet 将使用从 v1.9 版本开始提供的 apps/v1 API; Ingress 迁移到 networking.</description>
    </item>
    
    <item>
      <title>K8S 生态周报| 2019-07-08~2019-07-14</title>
      <link>https://moelove.info/2019/07/15/K8S-%E7%94%9F%E6%80%81%E5%91%A8%E6%8A%A5-2019-07-08~2019-07-14/</link>
      <pubDate>Mon, 15 Jul 2019 05:44:22 +0800</pubDate>
      
      <guid>https://moelove.info/2019/07/15/K8S-%E7%94%9F%E6%80%81%E5%91%A8%E6%8A%A5-2019-07-08~2019-07-14/</guid>
      <description>「K8S 生态周报」内容主要包含我所接触到的 K8S 生态相关的每周值得推荐的一些信息。欢迎订阅知乎专栏「k8s生态」。 本周为什么发布时间比往常迟呢？因为我在忙结婚呀。
 CoreDNS v1.5.2 发布 这是 CoreDNS 在 1.5.x 版本中发布的第二个小版本，关于 1.5.1 版本的说明可参考上上周的文章。
在此版本中，一个重要的变更便是移除掉了 upstream 插件相关的所有文档和说明。比如，Kubernetes 1.14 版本中默认的 CoreDNS 的配置文件的内容如下：
.:53 { errors health kubernetes cluster.local in-addr.arpa ip6.arpa { pods insecure upstream fallthrough in-addr.arpa ip6.arpa } prometheus :9153 forward . /etc/resolv.conf cache 30 loop reload loadbalance }  其中 kubernetes 插件中有一行 upstream 的配置，它是定义了用于解析指向外部主机的服务的上游解析器（也称之为外部服务，CNAME）CoreDNS 将针对自身解析该服务。
在此次变更之后， upstream 配置行便可直接移除。
另外 template 插件支持元数据了。比如说可以给它增加一个配置 .Meta &amp;quot;kubernetes/my-namespace&amp;quot;。
关于此版本的更详细说明可阅读 ReleaseNote
Envoy v1.</description>
    </item>
    
    <item>
      <title>K8S 生态周报| 2019-07-01~2019-07-07</title>
      <link>https://moelove.info/2019/07/07/K8S-%E7%94%9F%E6%80%81%E5%91%A8%E6%8A%A5-2019-07-01~2019-07-07/</link>
      <pubDate>Sun, 07 Jul 2019 23:20:10 +0800</pubDate>
      
      <guid>https://moelove.info/2019/07/07/K8S-%E7%94%9F%E6%80%81%E5%91%A8%E6%8A%A5-2019-07-01~2019-07-07/</guid>
      <description>「K8S 生态周报」内容主要包含我所接触到的 K8S 生态相关的每周值得推荐的一些信息。欢迎订阅知乎专栏「k8s生态」。
 Kubernetes v1.16 发布周期开始 随着前段时间 Kubernetes v1.15 的发布，v1.16 的发布周期开始了。本次的发布周期一如往常，本月底增强功能冻结，下月底代码冻结，9 月初完善文档，计划在 9 月中发布 v1.16 版本。
其实按这个节奏看得话，大家如果需要维护生产中的 Kubernetes 集群的话，还是尽快测试验证并完成升级，以免所用版本 EOL，带来一些其他的问题。
Knative Serving v0.7.x 发布 本周 Knative Serving 发布了 v0.7.1 版本，Knative 近期的开发还是比较活跃的。
需要注意的是若使用 v0.7.x 版本中新增的 serving.knative.dev/v1beta1 API 的话，则需要 Kubernetes v1.14 版本以上。具体原因请参考 #4533
Non-root 容器：在这个版本中所有发布的容器均以非 root 用户运行，这使得我们可以使用更严格的 PSP。
当然此版本中也包含一些破坏性变更，比如 status 字段废弃。
关于此版本更多的细节请参考 ReleaseNote
Debian 10 buster 正式发布 Debian 10 正式发布了，其实按一般的角度来看，Linux 的一个发行版发布不会出现在 K8S 生态周报中的。
但这里有个需要注意的点，对于使用此版本部署 Kubernetes 时，需要注意一下。此版本中使用的 systemd 版本是 241.</description>
    </item>
    
    <item>
      <title>K8S 生态周报| 2019-06-24~2019-06-30</title>
      <link>https://moelove.info/2019/06/30/K8S-%E7%94%9F%E6%80%81%E5%91%A8%E6%8A%A5-2019-06-24~2019-06-30/</link>
      <pubDate>Sun, 30 Jun 2019 23:12:13 +0800</pubDate>
      
      <guid>https://moelove.info/2019/06/30/K8S-%E7%94%9F%E6%80%81%E5%91%A8%E6%8A%A5-2019-06-24~2019-06-30/</guid>
      <description>「K8S 生态周报」内容主要包含我所接触到的 K8S 生态相关的每周值得推荐的一些信息。欢迎订阅知乎专栏「k8s生态」。
 kind (Kubernetes In Docker) v0.4.0 正式发布 kind (Kubernetes In Docker) 是我很喜欢并且一直持续参与贡献的项目，本周发布了 v0.4.0 版本。关于 Kind 的介绍和基础使用，可以参考我之前写的文章 《使用 Kind 搭建你的本地 Kubernetes 集群》
v0.4.0 版本中，默认的 Kubernetes 版本升级到了 v1.15 版本，且 kind.sigs.k8s.io/v1alpha2 版本的 API 已经过期，请更新使用 kind.sigs.k8s.io/v1alpha3 。
目前暂时移除了使用 apt 构建 Node 镜像的选项，之后版本中可能会加回来，直接使用上游构建好的二进制文件进行安装。
在此版本中，我们增加了一个 nodes[].extraPortMappings 的配置，可以直接通过此配置进行端口的转发，以便从宿主机上直接访问到集群上使用 NodePort 方式部署的服务，这样更容易模拟真实的网络环境，否则只能通过其他的转发或者网络代理的方式来进行通信了。
同样的，紧跟着上游的开放，这个版本中也增加了对 IPv6 的支持，可以直接通过 networking.ipFamily 的配置进行使用。
为了能让 kind 更加易用，且满足多数 CI 或者测试使用的场景，在这个版本中，我们尤其对单节点集群的启动时间做了优化，可以更快速的启动集群。
顺便公布一个数据，kind 目前的 star 数是 2.2k 上个版本发布时是 1.8k 并且还在持续增长中 :)
更多的细节和信息请参考 ReleaseNote 欢迎大家使用！</description>
    </item>
    
    <item>
      <title>K8S 生态周报| 2019-06-17~2019-06-23</title>
      <link>https://moelove.info/2019/06/22/K8S-%E7%94%9F%E6%80%81%E5%91%A8%E6%8A%A5-2019-06-17~2019-06-23/</link>
      <pubDate>Sat, 22 Jun 2019 23:23:17 +0800</pubDate>
      
      <guid>https://moelove.info/2019/06/22/K8S-%E7%94%9F%E6%80%81%E5%91%A8%E6%8A%A5-2019-06-17~2019-06-23/</guid>
      <description>「K8S 生态周报」内容主要包含我所接触到的 K8S 生态相关的每周值得推荐的一些信息。欢迎订阅知乎专栏「k8s生态」。
 Kubernetes v1.15.0 正式发布 经过了三个月左右的开发，Kubernetes v1.15.0 正式发布了。
这是 2019 年 Kubernetes 发布的第二个版本，这个版本由 25 个增强功能组成，其中 2 个移动到 stable ，13 个 beta 以及 10 个 alpha ，整体上集中于稳定性改进和扩展的增强。
CRD (Custom Resource Definition) 是 Kubernetes 提供的一种可用于扩展其能力的方式，当前有很多使用 CRD 构建于 Kubernetes 上的平台/系统，可以说之后对 Kubernetes 的扩展，或者说想要基于 Kubernetes 开发，同时又想与上游保持同步的话，CRD 是个最佳的选择。
Kubeadm 在此版本开始有了自己独立的 LOGO ，同时在这个版本中 kubeadm 的功能也得到了很多的完善和补充。这使得 kubeadm 成为更普遍/更好用的搭建集群的工具，同时对集群生命周期的管理也做的更加到位了。这部分的功能我很喜欢也一直在关注，近期我会针对这部分写篇文章出来。感兴趣的朋友们可以关注下
关于此版本更多的介绍，可参考 Kubernetes v1.15 ReleaseNote
Istio 1.2.0 正式发布 经过三个 rc 版本之后， Istio 1.2.0 版本正式发布。
在这个版本中，它添加了对 Kubernetes IPv6 的实验性支持。Kind （Kubernetes in Docker） 项目也是本周内刚增加了 IPv6 的支持 :)</description>
    </item>
    
    <item>
      <title>K8S 生态周报| 2019-06-10~2019-06-16</title>
      <link>https://moelove.info/2019/06/16/K8S-%E7%94%9F%E6%80%81%E5%91%A8%E6%8A%A5-2019-06-10~2019-06-16/</link>
      <pubDate>Sun, 16 Jun 2019 23:17:17 +0800</pubDate>
      
      <guid>https://moelove.info/2019/06/16/K8S-%E7%94%9F%E6%80%81%E5%91%A8%E6%8A%A5-2019-06-10~2019-06-16/</guid>
      <description>「K8S 生态周报」内容主要包含我所接触到的 K8S 生态相关的每周值得推荐的一些信息。欢迎订阅知乎专栏「k8s生态」。
 Helm 新下载域名正式上线 https://get.helm.sh/ 正式上线。用户之后下载 Helm 预编译好的二进制文件时，可通过此域名进行下载。
原来 Kubernetes 尚未成为 CNCF 托管项目时，Helm 是作为 Kubernetes 项目的一部分的，所以很自然的使用了 Google 的一个云存储仓库。但随着项目托管至 CNCF 以及后续 Helm 的独立发展，现在使用托管于 Google 的云存储不那么合适了，一方面在于 CNCF 正在接管 K8S 的基础设施，另一方面在于在于这个仓库不只受 Helm 的控制。
考虑到项目的独立性，以及 大陆用户无法正常访问 GCloud 存储的问题 经过维护者们的慎重考虑以及实际测试，终于决定选择 Azure 的 Blob 存储 + CDN 可满足当前所有地区的快速访问（尤其是国内可以直接访问并下载），不再需要花费时间精力解决网络等问题了。
这次的更改仅限于 Helm 客户端的下载位置，类似 Tiller 或者 Chart 等并没有被包含在内。
强烈建议更新有在 CI/自动化任务中使用的 Helm 下载地址，使用 https://get.helm.sh/ 来进行替换。
对此内容感兴趣的朋友可参考 Move Helm Downloads 的讨论
Apple 作为白金终端用户成员加入 CNCF Apple 在 K8S 社区中在这之前也算相对低调，并没有像各类云厂商或其他公司那样疯狂安利或者输出之类的。但是这次突然加入 CNCF 而且作为白金会员，是可具备话语权的。</description>
    </item>
    
    <item>
      <title>K8S 生态周报| 2019-06-03~2019-06-09</title>
      <link>https://moelove.info/2019/06/09/K8S-%E7%94%9F%E6%80%81%E5%91%A8%E6%8A%A5-2019-06-03~2019-06-09/</link>
      <pubDate>Sun, 09 Jun 2019 22:15:11 +0800</pubDate>
      
      <guid>https://moelove.info/2019/06/09/K8S-%E7%94%9F%E6%80%81%E5%91%A8%E6%8A%A5-2019-06-03~2019-06-09/</guid>
      <description>「K8S 生态周报」内容主要包含我所接触到的 K8S 生态相关的每周值得推荐的一些信息。欢迎订阅知乎专栏「k8s生态」。
 Kubernetes CVE-2019-11245 漏洞 这是一个评分为 4.9 的漏洞，算是一个中等漏洞。 受此漏洞影响的版本为 v1.13.6 和 v1.14.2 ，所以本周也加紧发布了 v1.13.7 和 v1.14.3 版本，以避免受此漏洞影响。 如果有使用 v1.13.6 和 v1.14.2 版本的小伙伴，请尽快进行 升级 以免受到影响。
上面说了最直接的解决办法，接下来对此漏洞大致做下介绍：
这个漏洞影响了 v1.13.6 和 v1.14.2 版本的 kubelet，具体表现为， 1) 如果 Pod 中的容器，开始时是以某个非 root 用户启动的，但是当它重启后，则会以 root (uid 0) 的身份启动。2) 或者是 Node 节点上已经存在了启动容器所需的镜像。
第 2 个情况比较常见，不再具体介绍。我们来看下第 1 种情况。举个栗子：
通常情况下，如果我们使用 Docker 官方的 Redis 镜像进行部署的时候，默认情况下将会以 redis 用户启动；而如果受此漏洞影响，当容器重启后，则当前的用户可能会变成 root (uid 0) 。使用 root 用户启动服务可能带来的危害，这里也不再多进行展开了。
也存在例外，比如已经显式的通过 runAsUser 指定了运行用户，则不会受到此漏洞影响。</description>
    </item>
    
    <item>
      <title>K8S 生态周报| 2019-05-27~2019-06-02</title>
      <link>https://moelove.info/2019/05/31/K8S-%E7%94%9F%E6%80%81%E5%91%A8%E6%8A%A5-2019-05-27~2019-06-02/</link>
      <pubDate>Fri, 31 May 2019 07:42:37 +0800</pubDate>
      
      <guid>https://moelove.info/2019/05/31/K8S-%E7%94%9F%E6%80%81%E5%91%A8%E6%8A%A5-2019-05-27~2019-06-02/</guid>
      <description>「K8S 生态周报」内容主要包含我所接触到的 K8S 生态相关的每周值得推荐的一些信息。欢迎订阅知乎专栏「k8s生态」。
 Kubernetes v1.15.0-beta.1 发布 随着 KubeCon EU 的结束，Kubernetes 的开发工作继续回归正常，本周相继发布了 v1.12.9 和 v1.15.0-beta.1。
随着 v1.15 的正式版临近，维护期的 Kubernetes 版本也将变成 1.12~1.15，请尽快升级。
这个版本的变化，等正式版发布时候再进行介绍好了，有兴趣可以先看 ReleaseNote
Docker v19.03.0-beta5 发布 按照正常规律 Docker 19.03 正式版也将在近期进行发布，而最近的所有测试版本中，其实变化比较大的东西主要在 构建系统 上；构建系统的升级可以使构建速度更快，同时也增加了更多的安全特性。
这次的 beta5 也是常规修复，有兴趣可以先看 ReleaseNote
Docker CVE-2018-15664 安全漏洞 在 5 月 29 日我看到了 CVE 的信息，这个漏洞会影响 Docker 的全部版本，漏洞攻击的主要途径是 docker cp 相关的操作。
但是不必太过紧张，因为这个漏洞的攻击范围其实不算太大；最主要可能被攻击的对象其实是公有云。对于普通用户而言，如果受此攻击，那前提是攻击者已经具备了机器的权限和 Docker 的操作权限（一般用户只要自行控制权限便可避免攻击的发生）。
漏洞发现者 Aleksa Sarai 开始提了一个 PR (他的实现方式是在 docker cp 操作的同时暂停容器)，不过现在已经被一个新的 PR 给取代了，毕竟暂停容器意味着停止服务，这是难以接受的。
类似 Podman 之类的其实也存在相同的问题，不过现在也已经被修复了。</description>
    </item>
    
    <item>
      <title>K8S 生态周报| 2019-05-20~2019-05-26</title>
      <link>https://moelove.info/2019/05/26/K8S-%E7%94%9F%E6%80%81%E5%91%A8%E6%8A%A5-2019-05-20~2019-05-26/</link>
      <pubDate>Sun, 26 May 2019 23:24:46 +0800</pubDate>
      
      <guid>https://moelove.info/2019/05/26/K8S-%E7%94%9F%E6%80%81%E5%91%A8%E6%8A%A5-2019-05-20~2019-05-26/</guid>
      <description>「K8S 生态周报」内容主要包含我所接触到的 K8S 生态相关的每周值得推荐的一些信息。欢迎订阅知乎专栏「k8s生态」。
 KubeCon EU 举办 2019 年第一个 KubeCon + CloudNativeCon 于 5 月 20 ~ 23 日在巴塞罗那成功举办，这次大会吸引了七千多名参会者远超去年的参会人数。 这也从另一个侧面反映了 Kubernetes 和云原生在大大的普及
在大会上宣布了不少值得关注的信息, 我在此大致列一下我认为值得关注的信息（虽然有些内容之前已经关注到了）：
 OpenTracing, OpenCensus 合并为 OpenTelemetry； 微软推出 Service Mesh Interface（SMI）规范； NGINX Ingress Controller 发布 1.5.0 版本； Google 宣布 GKE 将会支持 Windows Server Container； Helm 3 的发展历程；（推荐阅读我之前写的 初试 Helm 3）  当然，大会上公布的信息还有很多，还有一些 CNCF 的计划等，这里暂且不提，感兴趣的朋友可以自行搜索或者参加下个月在上海举办的 KubeCon + CloudNativeCon
微软推出 Service Mesh Interface （SMI） Service Mesh 也是一个趋势，但现在并没有一个统一的规范，各个厂商的实现也都各有不同。微软本次提出的 SMI 主要是为 Kubernetes 服务网格提供通用接口，以便能让 Service Mesh 有更加通用的规范 （就像当初 CNI/CRI 那样子）</description>
    </item>
    
    <item>
      <title>K8S 生态周报| 2019-05-13~2019-05-19</title>
      <link>https://moelove.info/2019/05/19/K8S-%E7%94%9F%E6%80%81%E5%91%A8%E6%8A%A5-2019-05-13~2019-05-19/</link>
      <pubDate>Sun, 19 May 2019 23:13:42 +0800</pubDate>
      
      <guid>https://moelove.info/2019/05/19/K8S-%E7%94%9F%E6%80%81%E5%91%A8%E6%8A%A5-2019-05-13~2019-05-19/</guid>
      <description>「K8S 生态周报」内容主要包含我所接触到的 K8S 生态相关的每周值得推荐的一些信息。欢迎订阅知乎专栏「k8s生态」。
 kind v0.3.0 正式发布 kind (Kubernetes In Docker) 是我很喜欢并且一直持续参与贡献的项目，本周发布了 v0.3.0 版本。关于 Kind 的介绍和基础使用，可以参考我之前写的文章 《使用 Kind 搭建你的本地 Kubernetes 集群》
本次的发布主要侧重于加速集群的启动速度及提高稳定性，优化镜像大小，以及对网络的优化和一些 bugfix 等；其中最主要的内容是将默认的 CRI 从 Docker 换成了 Containerd，以此可以缩小镜像体积，以及加快集群的启动。
v0.3.0 版本中，可以通过配置文件自行部署不同的 CNI，更有利于用户测试实际的集群情况；现在版本中已经将默认的 Kubernetes 版本升级到了最新的 v1.14.2 。
当然，也还有一些正在增加的特性，预计会在 v0.4.0 版本中发布，主要集中于 IPv6 和集群重启的支持（相信很快就可以完成了）。
顺便公布一个数据，Kind 目前的 star 数是 1.8k 还在持续增长中 :)
更多的细节和信息请参考 ReleaseNote
Kubernetes v1.14.2 正式发布 这是一个常规的 bugfix 版本，但有个值得关注的点：
 升级到了 golang v1.12.5 版本。  你可能要问为什么需要关注 golang 版本的升级？这是因为在此版本中 golang 有一些关于运行时的修改，尤其是其中关于二叉树查找部分的修改等部分的修改，可有效的降低 Kubernetes API server 的延迟。</description>
    </item>
    
    <item>
      <title>K8S 生态周报| 2019-05-06~2019-05-12</title>
      <link>https://moelove.info/2019/05/12/K8S-%E7%94%9F%E6%80%81%E5%91%A8%E6%8A%A5-2019-05-06~2019-05-12/</link>
      <pubDate>Sun, 12 May 2019 20:47:02 +0800</pubDate>
      
      <guid>https://moelove.info/2019/05/12/K8S-%E7%94%9F%E6%80%81%E5%91%A8%E6%8A%A5-2019-05-06~2019-05-12/</guid>
      <description>「K8S 生态周报」内容主要包含我所接触到的 K8S 生态相关的每周值得推荐的一些信息。欢迎订阅知乎专栏「k8s生态」。
 Alpine Linux Docker 镜像漏洞 CVE-2019-5021 本周比较吓人的是 CVE-2019-5021, 根据漏洞报告，自 Alpine Linux 3.3 版本开始的所有 Docker 镜像中，root 用户包含一个空密码，这可能会导致攻击者获得 root 权限，今儿造成攻击。
报告中称：受影响范围是 Alpine Linux Docker 镜像 3.3、3.4、3.5、3.6、3.7、3.8、3.9、edge 等全部版本。
要知道由于 Alpine Linux 镜像体积较小，所以在构建 Docker 镜像时，很多人都会推荐使用 Alpine Linux 作为基础镜像；包括很多 Docker 官方镜像也基本上都提供了基于 Alpine Linux 的镜像，甚至像 Docker 镜像等是只提供了使用 Alpine Linux 作为基础镜像的版本。
当前漏洞已经修复，更多内容请阅读 关于 Alpine Docker 镜像漏洞 CVE-2019-5021 。
istio-operator 发布 0.1.12 版本 banzaicloud/istio-operator 发布 0.1.12 版本，默认已支持 istio 1.1.5 。（虽然截至目前 istio 发布了 1.</description>
    </item>
    
    <item>
      <title>K8S 生态周报| 2019-04-28~2019-05-05</title>
      <link>https://moelove.info/2019/05/05/K8S-%E7%94%9F%E6%80%81%E5%91%A8%E6%8A%A5-2019-04-28~2019-05-05/</link>
      <pubDate>Sun, 05 May 2019 21:59:07 +0800</pubDate>
      
      <guid>https://moelove.info/2019/05/05/K8S-%E7%94%9F%E6%80%81%E5%91%A8%E6%8A%A5-2019-04-28~2019-05-05/</guid>
      <description>「K8S 生态周报」内容主要包含我所接触到的 K8S 生态相关的每周值得推荐的一些信息。欢迎订阅知乎专栏「k8s生态」。
 Docker Hub 只读维护 在上周的推送中，有写到 Docker Hub 用户隐私数据泄漏。受此事件影响，5 月 4 日 Docker Hub 进行升级维护，在此期间 Docker Hub 有一段时间处于只读模式，包括自动构建等服务不可用；在最后有小于 15 分钟的完全宕机时间，服务完全不可用。
如果只是看事情表面的话，可能这就是一个由于发现“安全问题”而进行的升级/维护；但如果仔细考虑下，作为云原生服务，升级为何会有宕机的情况，为何会有服务完全不可用的时候？
摘录一段来自本次维护的公告内容：
 Q: Is this maintenance related to recent Docker Hub data breach?
A: While we discovered unauthorized access to a single Hub database storing a subset of non-financial user data last week, which has since been remediated, we are always looking at ways to improve and enhance our security practices to protect our customers and their data.</description>
    </item>
    
    <item>
      <title>K8S 生态周报| 2019-04-22~2019-04-28</title>
      <link>https://moelove.info/2019/04/29/K8S-%E7%94%9F%E6%80%81%E5%91%A8%E6%8A%A5-2019-04-22~2019-04-28/</link>
      <pubDate>Mon, 29 Apr 2019 00:36:25 +0800</pubDate>
      
      <guid>https://moelove.info/2019/04/29/K8S-%E7%94%9F%E6%80%81%E5%91%A8%E6%8A%A5-2019-04-22~2019-04-28/</guid>
      <description>「K8S 生态周报」内容主要包含我所接触到的 K8S 生态相关的每周值得推荐的一些信息。欢迎订阅知乎专栏「k8s生态」。
 Docker Hub 用户隐私数据泄漏 2019 年 4 月 25 日，Docker Hub 团队发现了对存储非财务用户数据子集的单个 Hub 数据库的未授权访问。 在发现异常后官方团队迅速采取行动并保护网站免受攻击。
经过官方团队的调查，目前大概有 190000 帐号的敏感信息（小于总用户数的 5% ）包括用户名和哈希后的用户密码，当然也包括 GitHub 及 Bitbucket 等的用于自动构建的 Token 。
当前的主要措施是对可能被泄漏信息的用户发送了邮件通知，对于可能泄漏哈希密码的用户发送了重置密码的邮件，并且 主动 将密码失效，以及自动构建的 Token 也都被失效。( 所以如果你收到了 Docker Hub 团队关于此次事件的直接报告邮件，很大概率是因为你的信息已经被泄漏了 )
附上官方声明中关于此次事件的处理声明：
 During a brief period of unauthorized access to a Docker Hub database, sensitive data from approximately 190,000 accounts may have been exposed (less than 5% of Hub users).</description>
    </item>
    
    <item>
      <title>K8S 生态周报| 2019-04-15~2019-04-21</title>
      <link>https://moelove.info/2019/04/21/K8S-%E7%94%9F%E6%80%81%E5%91%A8%E6%8A%A5-2019-04-15~2019-04-21/</link>
      <pubDate>Sun, 21 Apr 2019 21:46:02 +0800</pubDate>
      
      <guid>https://moelove.info/2019/04/21/K8S-%E7%94%9F%E6%80%81%E5%91%A8%E6%8A%A5-2019-04-15~2019-04-21/</guid>
      <description>「K8S 生态周报」内容主要包含我所接触到的 K8S 生态相关的每周值得推荐的一些信息。欢迎订阅知乎专栏「k8s生态」。
 Prometheus v2.9.0 正式发布 Prometheus 是 CNCF 毕业项目，可用于监控系统及服务状态。它整体是使用 Pull 的模式，在周期时间内采集目标的 metrics ，并且提供了 PromQL 的查询语言，以供对监控数据进行查询过滤等操作。并且可以通过配置规则来触发报警等。我首次接触 Prometheus 大概是在 2015 年 0.15.0 版本左右，当时 Prometheus 还处于比较早期的阶段，不过在进入 CNCF 后，Prometheus 基本就成为了 K8S 监控的实施标准了，并且多数软件也都增加了对 Prometheus metrics 的支持。
v2.9.0 的主要更新：
 从 2.8 开始引入了的从 WAL 读取进行 remote write 有时候会丢数据的问题已经得到修复； Kubernetes 和 OpenStack 在服务发现时候增加了更多元数据； Consul 现在支持多 tag； 添加了一个 honor_timestamps 的选项； TLS 证书会自动从磁盘加载； 日志也变的更易读；  其他更新请阅读 ReleaseNote
Linkerd 2.3 正式发布 Linkerd 是一个 service mesh 旨在提供平台范围的可观察性，可靠性和安全性，而无需用户更改代码。在本月初的周报推送中，推荐了一篇关于 Linkerd v2 从产品中吸取的教育和经验的文章，Linkerd v2 使用 Go 和 Rust 进行了重写，并因此获得了巨大的收益。</description>
    </item>
    
    <item>
      <title>K8S 生态周报| 2019.04.08~2019.04.14</title>
      <link>https://moelove.info/2019/04/14/K8S-%E7%94%9F%E6%80%81%E5%91%A8%E6%8A%A5-2019.04.08~2019.04.14/</link>
      <pubDate>Sun, 14 Apr 2019 22:53:09 +0800</pubDate>
      
      <guid>https://moelove.info/2019/04/14/K8S-%E7%94%9F%E6%80%81%E5%91%A8%E6%8A%A5-2019.04.08~2019.04.14/</guid>
      <description>「K8S 生态周报」内容主要包含我所接触到的 K8S 生态相关的每周值得推荐的一些信息。欢迎订阅知乎专栏「k8s生态」。
 CRI-O 成为 CNCF 托管项目 CRI-O 是基于 OCI 的 Kubernetes CRI 实现，旨在提供符合 OCI 运行时和 kubelet 之间的集成。简单来说就是完全符合 OCI 标准的 CRI 实现。（比如之前介绍的 runc 便是 OCI 标准的参考实现）
在 2016 年的时候 Kubernetes 就推出了容器运行时接口（CRI），这给了 kubelet 一种使用各种不同容器运行时的能力，现在最常用的当然还是 Docker，当然也有人使用 containerd、runc、CRI-O 等各类运行时。
CRI-O 最初由 Red Hat 和 Google 开发，现在已达到稳定状态，且已有大量的贡献者，本次成为 CNCF 托管项目，也算是给容器运行时提供一个更大的可能。
附一张官方图：
详细信息请阅读 CNCF 官方新闻
Helm 子项目 chart-testing 发布 v2.3.0 版本 chart-testing v2.3.0 版本正式发布，该项目的主要目标是用于 Helm Chart 的测试，使用该项目可更方便的检查 Chart 中是否有错误，以及定位错误位置等。
本次发布主要在于覆盖更多异常情况，详细内容建议阅读 ReleaseNote
CoreDNS v1.</description>
    </item>
    
    <item>
      <title>K8S 生态周报| 2019.04.01~2019.04.07</title>
      <link>https://moelove.info/2019/04/07/K8S-%E7%94%9F%E6%80%81%E5%91%A8%E6%8A%A5-2019.04.01~2019.04.07/</link>
      <pubDate>Sun, 07 Apr 2019 10:03:13 +0800</pubDate>
      
      <guid>https://moelove.info/2019/04/07/K8S-%E7%94%9F%E6%80%81%E5%91%A8%E6%8A%A5-2019.04.01~2019.04.07/</guid>
      <description>「K8S 生态周报」内容主要包含我所接触到的 K8S 生态相关的每周值得推荐的一些信息。欢迎订阅知乎专栏「k8s生态」。
 Kubernetes client-go v11.0.0 正式发布 这是最后一个使用 dep 作为依赖管理的版本，后续版本将转向使用 go modules.
Kubernetes 生态中的相关项目大多都已转向或正在转向使用 go modules 了，这也是一个技术风向，理性选择。
Release
containerd 1.2.6 正式发布 这是 containerd 1.2 的第 6 个 patch 版本，主要更新：
 在默认的 seccomp profile 白名单增加了 io_pgetevents 和 statx 这两个系统调用; 修复了在 1.2.5 中自定义 cgroup path 无法工作的 bug； 更新 CNI 插件到 v0.7.5 以修复 CVE-2019-9946; 更新 runc 版本，修复在无 SELinux 系统下的失败情况；  当然还有一些其他的改进和修复，比如修复了 pod 的 UTS namespace 等，建议阅读 ReleaseNote。
Docker CE 19.</description>
    </item>
    
    <item>
      <title>K8S 生态周报| 2019.03.25~2019.03.31</title>
      <link>https://moelove.info/2019/03/31/K8S-%E7%94%9F%E6%80%81%E5%91%A8%E6%8A%A5-2019.03.25~2019.03.31/</link>
      <pubDate>Sun, 31 Mar 2019 21:52:01 +0800</pubDate>
      
      <guid>https://moelove.info/2019/03/31/K8S-%E7%94%9F%E6%80%81%E5%91%A8%E6%8A%A5-2019.03.25~2019.03.31/</guid>
      <description>「K8S 生态周报」内容主要包含我所接触到的 K8S 生态相关的每周值得推荐的一些信息。欢迎订阅知乎专栏「k8s生态」。
 Kubernetes 1.14 正式发布 1.14 的主要更新：
 对 Windows Node 和 container 的支持达到生产级别，支持 Windows Server 2019； 本地持久化数据卷正式可用，这可以方便使用本地 SSD 之类的存储，但注意这个特性容错性较差； Pod 优先级和抢占机制正式可用，(建议慎重使用)； Pod Ready++ (Pod Readiness Gates) 达到稳定，可以更好的判断 Pod 及其需要的资源是否均已就绪；  当然还有很多的改进和很多被废弃的功能特性等，建议阅读 ReleaseNote。
Minikube 1.0.0 正式发布 Minikube 是一个用于本地搭建 Kubernetes 环境的工具，使用方法可参考 使用 Minikube 搭建本地 Kubernetes 环境。
1.0.0 的主要更新：
 默认 Kubernetes 版本更新至 1.14.0; 新增 --image-repository 参数，方便国内用户使用镜像解决网络问题；  其他特性请阅读 ReleaseNote
runc 1.0-rc7 发布 注意，低版本内核(尤其是 3.x)的系统，请不要升级至此版本
这个版本主要为解决之前的漏洞及修正一些规范等，版本说明请参考 runc 1.</description>
    </item>
    
    <item>
      <title>K8S 生态周报| 2019.03.18~2019.03.24</title>
      <link>https://moelove.info/2019/03/25/K8S-%E7%94%9F%E6%80%81%E5%91%A8%E6%8A%A5-2019.03.18~2019.03.24/</link>
      <pubDate>Mon, 25 Mar 2019 20:49:06 +0800</pubDate>
      
      <guid>https://moelove.info/2019/03/25/K8S-%E7%94%9F%E6%80%81%E5%91%A8%E6%8A%A5-2019.03.18~2019.03.24/</guid>
      <description>我将从本篇开始维护「K8S 生态周报」内容主要包含我所接触到的 K8S 生态相关的每周值得推荐的一些信息。
 Docker 6 岁啦 Docker 从 2013 年首次亮相，至今已 6 年之久，而 Docker 也已一度成为容器技术的代名词，很庆幸能投身 Docker 相关的领域。官方博客
Kind (Kubernetes In Docker) 发布 0.2.0 版本 Kind 是一个利用容器技术快速部署本地 Kubernetes 的工具，主要是用于对 Kubernetes 1.11+ 版本的测试。现在发布的 0.2.0 版本支持最新 Kubernetes v1.13.4 及 Docker 18.06.3 且通过了 CNCF 的一致性认证。
Rancher 发布 K8S 最佳安全实践文章 Rancher 在 CNCF 最近发布的 9 个 Kubernetes 最佳安全实践的基础上发布了一篇更安全的最佳实践，这两篇文章都值得一看。
可以通过下面二维码订阅我的文章公众号【MoeLove】</description>
    </item>
    
  </channel>
</rss>